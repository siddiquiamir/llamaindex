{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2fe244-8fd6-4288-9ba3-8b8226a61b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create api key here\n",
    "# https://cloud.llamaindex.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0050d-efee-4611-8135-002f355f1a85",
   "metadata": {},
   "source": [
    "# LlamaParse\n",
    "- LlamaParse is an API created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n",
    "\n",
    "#### NOTE: Currently, only PDF files are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650183a-c8a3-477e-88b4-d4ddc5cb3317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45a500b-b761-41be-adf1-42fe47535cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b75daf0-3959-4420-a176-68c964abff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba053aa-1150-4ffd-9008-3c3eac0df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"paste api key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d00cc0-e624-472f-af10-44adbdcd50c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 34 2163k   34  744k    0     0   332k      0  0:00:06  0:00:02  0:00:04  332k\n",
      "100 2163k  100 2163k    0     0   900k      0  0:00:02  0:00:02 --:--:--  901k\n"
     ]
    }
   ],
   "source": [
    "!curl -o attention.pdf \"https://arxiv.org/pdf/1706.03762.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72491a3-6e95-4cc7-8d39-4d040ef6b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id e223d6d9-85da-452c-b6cd-699f7679f3c9\n"
     ]
    }
   ],
   "source": [
    "# llama-parse is async-first, running the sync code in a notebook requires the use of nest_asyncio\n",
    "# As a text result type\n",
    "from llama_parse import LlamaParse\n",
    "documents = LlamaParse(result_type=\"text\").load_data(\"./attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b1e4f3b-dd2c-49f3-9d05-ac4ffd686a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2    Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective res\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text[6000:7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4251c14a-fb7a-4dff-a613-9ca08d5bbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 6d633a95-b0fb-40b2-999d-396e2c137f01\n"
     ]
    }
   ],
   "source": [
    "# As a markdown result type\n",
    "from llama_parse import LlamaParse\n",
    "documents = LlamaParse(result_type=\"markdown\").load_data(\"./attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41cf125-3ee9-48a5-9769-5aee637c531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "\n",
      "### Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,...\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text[20000:21000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e75d4-2cd3-41c5-be91-e7e5e2c2da99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
