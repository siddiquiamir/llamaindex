{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5aee99-6a79-4661-be42-5c5b1b16a0de",
   "metadata": {},
   "source": [
    "# Storing\n",
    "- Once you have data loaded and indexed, you will probably want to store it to avoid the time and cost of re-indexing it. By default, your indexed data is stored only in memory.\n",
    "\n",
    "# Persisting to disk\n",
    "- The simplest way to store your indexed data is to use the built-in .persist() method of every Index, which writes all the data to disk at the location specified. This works for any type of index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ce5817-d3b6-4f40-80b2-e9b21de36a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380a0ea7-4e93-46be-816f-855c1b222a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f98010e-5a01-4f47-b563-b22f23db16cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceInferenceAPI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001F8F3B3FAD0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001F8FEFB7420>, completion_to_prompt=<function default_completion_to_prompt at 0x000001F8FF045B20>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model_name='mistralai/Mixtral-8x7B-Instruct-v0.1', token='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', timeout=None, headers=None, cookies=None, task=None, context_window=3900, num_output=256, is_chat_model=False, is_function_calling_model=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create llm model\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=HF_TOKEN)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b703ce8c-e5cd-4499-865c-9ea65e5da46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1- load text file\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "reader = SimpleDirectoryReader(input_files= [\"paul_essay.txt\"])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb94e1a2-0fb7-4fc3-a43c-e48002060a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abffe2012f74ff8967b695be2aa120c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd2462098ba472ca2b5136d6adc800c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4429d5a-4761-43b6-bf68-76eff900f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's store indexing in the database db_indexing\n",
    "persist_dir = \"./db_indexing\"\n",
    "index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedde02e-4b27-4a91-934b-622d4946a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then avoid re-loading and re-indexing your data by loading the persisted index like this:\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir= persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bba5249-bb36-4248-a22a-bca3af094026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x1f883f66f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load index\n",
    "index_new = load_index_from_storage(storage_context, embed_model='local')\n",
    "index_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b8bf6e-9ce4-41c2-aa8d-13c276cc85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3- # configure retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index_new,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14eb232-54ad-4f53-a5b6-afcb38b1d400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x1f8f3b3fad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query your data\n",
    "query_engine = index_new.as_query_engine(llm=llm)\n",
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367d416d-ff1b-4a0d-bc62-dd6b7f41d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who is the author?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9a5af6-707e-4539-8f4a-37b6adee3161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The essay is about Paul Graham, his life, and his experiences with programming and writing. It covers his early experiences with programming on an IBM 1401, his switch to microcomputers, and his eventual career as a programmer and writer. The essay also discusses his experiences with Lisp and his development of the programming language Arc. Additionally, it covers his experiences with publishing essays online and the impact it had on his career and writing. The essay also mentions his relationship with Jessica Livingston.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who is the essay about?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0433a1-a9b7-4eba-8e30-7d873047f810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
