{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df6d280-60f0-4a34-9615-bacb7e35b16c",
   "metadata": {},
   "source": [
    "# Transformations - SentenceSplitter\n",
    "- After the data is loaded, you then need to process and transform your data before putting it into a storage system. \n",
    "- These transformations include chunking, extracting metadata, and embedding each chunk. This is necessary to make sure that the data can be retrieved,\n",
    "- and used optimally by the LLM.\n",
    "\n",
    "- Transformation input/outputs are Node objects (a Document is a subclass of a Node). Transformations can also be stacked and reordered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28123088-cc95-4eba-81e7-88165507f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c36c456-b4ee-426f-b6eb-d8b184b2dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26eb4e9d-8d38-499a-9635-df7d24ec31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\amirs\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252912c6-3291-47cf-b8ea-30e8f29408a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceInferenceAPI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002BF10106C10>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002BF1B8B2D40>, completion_to_prompt=<function default_completion_to_prompt at 0x000002BF1B945440>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model_name='mistralai/Mixtral-8x7B-Instruct-v0.1', token='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', timeout=None, headers=None, cookies=None, task=None, context_window=3900, num_output=256, is_chat_model=False, is_function_calling_model=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create llm model\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=HF_TOKEN)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b83e9b2-8bcd-420c-80f5-352b063af550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "reader = SimpleDirectoryReader(input_files= [\"llamaindex.txt\"])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8d67ba-7476-4a63-bc2d-b41feac3f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3ec508-7e17-47e6-b50b-2240396602e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba5640e-7ee5-43ab-899d-29a36d3b8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "282a353c-5609-420c-b433-4d31339c8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[text_splitter],embed_model='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e61d77-bbd3-41d0-b436-1ec7fd2e338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0915e9-c840-43e7-9fe0-df2f192e70da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LlamaIndex is an advanced and versatile toolkit written in Python for building large-scale knowledge graphs powered by artificial intelligence models. It facilitates rapid development and deployment of intelligent applications, offering features like scalable data ingestion, modular architecture, integration with leading AI models, high customizability, and extensive plugin support.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is llamaindex?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cce0c-53d6-42a3-8031-69eebca811c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
